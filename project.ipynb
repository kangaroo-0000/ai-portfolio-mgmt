{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Student Name: Chung-Mou Pan\n",
    "### Student Net Id: N14124164\n",
    "### All code in this file is written by myself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import time\n",
    "import requests\n",
    "from datetime import timedelta\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# STEP 1: LOAD AND CLEAN CONGRESSIONAL TRADE DATA\n",
    "# ====================================\n",
    "df = pd.read_excel('congress-trading-all.xlsx')\n",
    "df['Traded'] = pd.to_datetime(df['Traded'])\n",
    "\n",
    "# Convert 'Trade_Size_USD' from strings to a numeric approximation:\n",
    "# If it's in ranges like \"$1,001 - $15,000\", take the midpoint.\n",
    "def parse_trade_size(trade_str):\n",
    "    # Remove $, commas and split by '-'\n",
    "    if pd.isna(trade_str) or trade_str.strip() == '':\n",
    "        return np.nan\n",
    "    parts = trade_str.replace('$', '').replace(',', '').split('-')\n",
    "    if len(parts) == 2:\n",
    "        # Take midpoint\n",
    "        try:\n",
    "            low = float(parts[0])\n",
    "            high = float(parts[1])\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "        return (low + high) / 2\n",
    "    else:\n",
    "        # If not a range, try directly converting\n",
    "        return float(parts[0])\n",
    "\n",
    "df['Trade_Size_Mid'] = df['Trade_Size_USD'].apply(parse_trade_size)\n",
    "\n",
    "# Filter out rows without a valid ticker or date if necessary\n",
    "df = df.dropna(subset=['Ticker', 'Traded'])\n",
    "\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# STEP 2: EXTRACT TICKERS AND GET HISTORICAL PRICE DATA USING ALPACA\n",
    "# ====================================\n",
    "\n",
    "# Alpaca API credentials\n",
    "API_KEY = 'YOUR_API_KEY'\n",
    "SECRET_KEY = 'YOUR_SECRET_KEY' \n",
    "\n",
    "# Base URL for Alpaca's data API\n",
    "BASE_URL = 'https://data.alpaca.markets/v2/stocks/bars'\n",
    "\n",
    "def fetch_historical_bars(symbol, timeframe='1Day', start=None, end=None, limit=10000):\n",
    "    headers = {\n",
    "        'APCA-API-KEY-ID': API_KEY,\n",
    "        'APCA-API-SECRET-KEY': SECRET_KEY\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'symbols': symbol,\n",
    "        'timeframe': timeframe,\n",
    "        'start': start,\n",
    "        'end': end,\n",
    "        'limit': limit\n",
    "    }\n",
    "\n",
    "    response = requests.get(BASE_URL, headers=headers, params=params)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        if 'bars' in data and symbol in data['bars']:\n",
    "            bars = pd.DataFrame(data['bars'][symbol])\n",
    "            # Convert timestamp t to datetime\n",
    "            bars['t'] = pd.to_datetime(bars['t'], utc=True)\n",
    "            # Set datetime as the index\n",
    "            bars.set_index('t', inplace=True)\n",
    "            return bars\n",
    "        else:\n",
    "            print(f\"No data found for symbol {symbol}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(f\"Error {response.status_code}: {response.text}\")\n",
    "        return None\n",
    "\n",
    "# Get unique tickers\n",
    "tickers = df['Ticker'].dropna().unique()\n",
    "tickers = [t.strip().upper() for t in tickers if isinstance(t, str) and t.strip() != '']\n",
    "\n",
    "start_dt = df['Traded'].min() - timedelta(days=365)\n",
    "end_dt = pd.Timestamp.today()\n",
    "start_str = start_dt.strftime('%Y-%m-%dT00:00:00Z')\n",
    "end_str = end_dt.strftime('%Y-%m-%dT00:00:00Z')\n",
    "\n",
    "all_data = []\n",
    "\n",
    "# Download each tickerâ€™s data individually\n",
    "for ticker in tickers:\n",
    "    try:\n",
    "        print(f\"Downloading data for {ticker}...\")\n",
    "        bars = fetch_historical_bars(ticker, timeframe='1Day', start=start_str, end=end_str)\n",
    "        if bars is not None and not bars.empty:\n",
    "            # Add a column for the ticker symbol\n",
    "            bars['Ticker'] = ticker\n",
    "            all_data.append(bars)\n",
    "        else:\n",
    "            print(f\"No data found for {ticker}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "# Combine all individual DataFrames into one\n",
    "if all_data:\n",
    "    full_price_data = pd.concat(all_data, axis=0)\n",
    "    # Sort by index (datetime), then forward fill\n",
    "    full_price_data = full_price_data.sort_index().ffill()\n",
    "    print(\"Historical price data download complete.\")\n",
    "    print(full_price_data.head())\n",
    "else:\n",
    "    full_price_data = pd.DataFrame()\n",
    "    print(\"No data collected.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "full_price_data.to_pickle('price_data.pkl')\n",
    "\n",
    "# Save to CSV\n",
    "full_price_data.to_csv('historical_price_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# STEP 3: JOIN CONGRESSIONAL TRADE DATA WITH HISTORICAL PRICE DATA\n",
    "# ====================================\n",
    "\n",
    "# 1. Pivot price data to wide format so that rows = dates, columns = tickers, and values = close prices (c)\n",
    "pivoted_close = full_price_data.reset_index().pivot(index='t', columns='Ticker', values='c')\n",
    "\n",
    "# Forward fill missing values\n",
    "pivoted_close = pivoted_close.ffill()\n",
    "\n",
    "# save the pivoted data\n",
    "pivoted_close.to_csv('pivoted_close.csv')\n",
    "\n",
    "# 2. Ensure df's Ticker is upper case and stripped of whitespace\n",
    "df['Ticker'] = df['Ticker'].str.upper().str.strip()\n",
    "df['Traded'] = df['Traded'].dt.tz_localize('UTC') \n",
    "\n",
    "# 3. For each trade in df, find the last available price on or before the trade date\n",
    "merged_list = []\n",
    "for idx, row in df.iterrows():\n",
    "    ticker = row['Ticker']\n",
    "    trade_date = row['Traded']\n",
    "    # Check if ticker is in pivoted columns\n",
    "    if ticker in pivoted_close.columns:\n",
    "        # Find dates in price data up to trade_date\n",
    "        available_dates = pivoted_close.index[pivoted_close.index <= trade_date]\n",
    "        if len(available_dates) > 0:\n",
    "            # The closest date is the last element\n",
    "            closest_date = available_dates[-1]\n",
    "            trade_price = pivoted_close.loc[closest_date, ticker]\n",
    "        else:\n",
    "            trade_price = np.nan\n",
    "    else:\n",
    "        trade_price = np.nan\n",
    "    merged_list.append(trade_price)\n",
    "\n",
    "df['Trade_Price'] = merged_list\n",
    "\n",
    "# print the merged data\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "df.to_csv('merged_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# STEP 4: CALCULATE SENTIMENT METRICS\n",
    "# ====================================\n",
    "\n",
    "# Ensure 'df' has 'Traded' as datetime\n",
    "df['Traded'] = pd.to_datetime(df['Traded'])\n",
    "\n",
    "# Create a Buy/Sell Indicator\n",
    "# Purchase = +1, Sale = -1. If there is any other type, treat as 0.\n",
    "df['Buy_Sell_Indicator'] = df['Transaction'].apply(lambda x: 1 if x.lower() == 'purchase' else (-1 if x.lower() == 'sale' else 0))\n",
    "\n",
    "# Weighted Sentiment: multiply by Trade_Size_Mid to emphasize trade size\n",
    "df['Weighted_Sentiment'] = df['Buy_Sell_Indicator'] * df['Trade_Size_Mid']\n",
    "\n",
    "# Party-based sentiment:\n",
    "df['Party_Buy_Sell_D'] = df.apply(lambda row: row['Buy_Sell_Indicator'] if row['Party'] == 'D' else 0, axis=1)\n",
    "df['Party_Buy_Sell_R'] = df.apply(lambda row: row['Buy_Sell_Indicator'] if row['Party'] == 'R' else 0, axis=1)\n",
    "\n",
    "# Aggregate at the daily level per Ticker\n",
    "daily_sentiment = df.groupby(['Traded', 'Ticker']).agg(\n",
    "    net_sentiment=('Buy_Sell_Indicator', 'sum'),           # Net number of buys minus sells\n",
    "    weighted_sentiment=('Weighted_Sentiment', 'sum'),      # Sum of weighted sentiments\n",
    "    net_sentiment_D=('Party_Buy_Sell_D', 'sum'),           # Net sentiment from Democrats\n",
    "    net_sentiment_R=('Party_Buy_Sell_R', 'sum'),           # Net sentiment from Republicans\n",
    "    total_trades=('Buy_Sell_Indicator', 'count'),          # How many trades were made that day for that ticker\n",
    "    avg_trade_size=('Trade_Size_Mid', 'mean'),             # Average trade size that day\n",
    ").reset_index()\n",
    "\n",
    "print(daily_sentiment.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "daily_sentiment.to_csv('daily_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering and Model Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1 (Technical Indicators Dominant)\n",
    "# ====================================\n",
    "# STEP 1: PREPARE DATA\n",
    "# ====================================\n",
    "\n",
    "# Future returns: next 5 days horizon\n",
    "future_horizon = 5\n",
    "future_returns = pivoted_close.shift(-future_horizon) / pivoted_close - 1.0\n",
    "\n",
    "# Melt future_returns to long format\n",
    "future_returns_long = future_returns.stack().reset_index()\n",
    "future_returns_long.columns = ['Traded', 'Ticker', 'Future_Return']\n",
    "\n",
    "# Ensure ticker and date formatting matches sentiment data\n",
    "sentiment_df = daily_sentiment.reset_index() if not all(col in daily_sentiment.columns for col in ['Traded', 'Ticker']) else daily_sentiment\n",
    "sentiment_df['Ticker'] = sentiment_df['Ticker'].str.upper().str.strip()\n",
    "future_returns_long['Ticker'] = future_returns_long['Ticker'].str.upper().str.strip()\n",
    "\n",
    "# Normalize/truncate dates\n",
    "sentiment_df['Traded'] = sentiment_df['Traded'].dt.normalize()\n",
    "future_returns_long['Traded'] = future_returns_long['Traded'].dt.normalize()\n",
    "\n",
    "# Merge sentiment and future returns\n",
    "merged = pd.merge(sentiment_df, future_returns_long, on=['Traded', 'Ticker'], how='inner').dropna(subset=['Future_Return'])\n",
    "\n",
    "# ====================================\n",
    "# STEP 2: ADD TECHNICAL INDICATORS\n",
    "# ====================================\n",
    "\n",
    "# Copy pivoted_close for feature generation\n",
    "price_features = pivoted_close.copy()\n",
    "daily_ret = price_features.pct_change()\n",
    "\n",
    "# Calculate technical indicators\n",
    "mom_5d = daily_ret.rolling(window=5).mean()  # 5-day momentum\n",
    "mom_20d = daily_ret.rolling(window=20).mean()  # 20-day momentum\n",
    "vol_20d = daily_ret.rolling(window=20).std()  # 20-day volatility\n",
    "ma_20d = price_features.rolling(window=20).mean()  # 20-day moving average\n",
    "\n",
    "# RSI (Relative Strength Index)\n",
    "def compute_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = np.maximum(delta, 0)\n",
    "    loss = -np.minimum(delta, 0)\n",
    "    avg_gain = gain.rolling(window=period).mean()\n",
    "    avg_loss = loss.rolling(window=period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "rsi_14 = price_features.apply(compute_rsi, period=14)\n",
    "\n",
    "# MACD and Signal Line\n",
    "ema_12 = price_features.ewm(span=12, adjust=False).mean()\n",
    "ema_26 = price_features.ewm(span=26, adjust=False).mean()\n",
    "macd = ema_12 - ema_26\n",
    "signal_line = macd.ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# Bollinger Bands\n",
    "bollinger_upper = price_features.rolling(window=20).mean() + 2 * price_features.rolling(window=20).std()\n",
    "bollinger_lower = price_features.rolling(window=20).mean() - 2 * price_features.rolling(window=20).std()\n",
    "\n",
    "# Average True Range (ATR)\n",
    "def compute_atr(df, window=14):\n",
    "    high_low = df.diff().abs()\n",
    "    atr = high_low.rolling(window).mean()\n",
    "    return atr\n",
    "\n",
    "atr_14 = compute_atr(price_features)\n",
    "\n",
    "# Stack features and merge into `merged`\n",
    "def stack_feature(feat_df, col_name):\n",
    "    df_long = feat_df.stack().reset_index()\n",
    "    df_long.columns = ['Traded', 'Ticker', col_name]\n",
    "    df_long['Traded'] = df_long['Traded'].dt.normalize()\n",
    "    return df_long\n",
    "\n",
    "features_to_add = [\n",
    "    (mom_5d, 'mom_5d'), (mom_20d, 'mom_20d'), (vol_20d, 'vol_20d'), (ma_20d, 'ma_20d'),\n",
    "    (rsi_14, 'rsi_14'), (macd, 'macd'), (signal_line, 'macd_signal'),\n",
    "    (bollinger_upper, 'bollinger_upper'), (bollinger_lower, 'bollinger_lower'),\n",
    "    (atr_14, 'atr_14')\n",
    "]\n",
    "\n",
    "for feature, name in features_to_add:\n",
    "    stacked_feature = stack_feature(feature, name)\n",
    "    merged = pd.merge(merged, stacked_feature, on=['Traded', 'Ticker'], how='left')\n",
    "\n",
    "merged = merged.dropna()\n",
    "\n",
    "# ====================================\n",
    "# STEP 3: SELECT FEATURES AND TARGET\n",
    "# ====================================\n",
    "feature_cols = [\n",
    "    'net_sentiment', 'weighted_sentiment', 'net_sentiment_D', 'net_sentiment_R',\n",
    "    'total_trades', 'avg_trade_size', 'mom_5d', 'mom_20d', 'vol_20d', 'ma_20d',\n",
    "    'rsi_14', 'macd', 'macd_signal', 'bollinger_upper', 'bollinger_lower', 'atr_14'\n",
    "]\n",
    "target_col = 'Future_Return'\n",
    "\n",
    "X = merged[feature_cols]\n",
    "y = merged[target_col]\n",
    "\n",
    "# ====================================\n",
    "# STEP 4: TIME SERIES SPLIT AND SCALING\n",
    "# ====================================\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ====================================\n",
    "# STEP 5: MODEL TRAINING\n",
    "# ====================================\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, 15, None],\n",
    "    'min_samples_leaf': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_features': ['sqrt', 'log2', None], \n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='r2', cv=tscv, n_jobs=-1, verbose=1, error_score='raise')\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "print(\"Best Params:\", grid_search.best_params_)\n",
    "print(\"Best CV R^2 Score:\", grid_search.best_score_)\n",
    "\n",
    "# ====================================\n",
    "# STEP 6: FINAL MODEL EVALUATION\n",
    "# ====================================\n",
    "cutoff_date = pd.to_datetime(\"2022-12-31\")\n",
    "merged['Traded'] = merged['Traded'].dt.tz_localize(None)\n",
    "train_mask = merged['Traded'] < cutoff_date\n",
    "test_mask = merged['Traded'] >= cutoff_date\n",
    "\n",
    "X_train, y_train = X[train_mask], y[train_mask]\n",
    "X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "# Scale the train-test data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "final_model = RandomForestRegressor(**grid_search.best_params_, random_state=42)\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "y_pred = final_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Test R^2 Score with improved setup:\", r2_score(y_test, y_pred))\n",
    "print(\"Feature Importances:\", dict(zip(feature_cols, final_model.feature_importances_)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2 (Congressional Sentiment Dominant)\n",
    "# ====================================\n",
    "# STEP 1: PREPARE DATA\n",
    "# ====================================\n",
    "future_horizon = 5\n",
    "future_returns = pivoted_close.shift(-future_horizon) / pivoted_close - 1.0\n",
    "\n",
    "# Melt future_returns to long format\n",
    "future_returns_long = future_returns.stack().reset_index()\n",
    "future_returns_long.columns = ['Traded', 'Ticker', 'Future_Return']\n",
    "\n",
    "# Ensure ticker and date formatting matches sentiment data\n",
    "sentiment_df = daily_sentiment.reset_index() if not all(col in daily_sentiment.columns for col in ['Traded','Ticker']) else daily_sentiment\n",
    "sentiment_df['Ticker'] = sentiment_df['Ticker'].str.upper().str.strip()\n",
    "future_returns_long['Ticker'] = future_returns_long['Ticker'].str.upper().str.strip()\n",
    "\n",
    "sentiment_df['Traded'] = sentiment_df['Traded'].dt.normalize()\n",
    "future_returns_long['Traded'] = future_returns_long['Traded'].dt.normalize()\n",
    "\n",
    "merged = pd.merge(sentiment_df, future_returns_long, on=['Traded', 'Ticker'], how='inner').dropna(subset=['Future_Return'])\n",
    "\n",
    "# ====================================\n",
    "# STEP 2: ADD TECHNICAL INDICATORS (REDUCED)\n",
    "# ====================================\n",
    "price_features = pivoted_close.copy()\n",
    "daily_ret = price_features.pct_change()\n",
    "\n",
    "# Keep only minimal technical indicators\n",
    "mom_5d = daily_ret.rolling(window=5).mean()    # 5-day momentum\n",
    "mom_20d = daily_ret.rolling(window=20).mean()  # 20-day momentum\n",
    "vol_20d = daily_ret.rolling(window=20).std()   # 20-day volatility\n",
    "ma_20d = price_features.rolling(window=20).mean() # 20-day MA\n",
    "\n",
    "def stack_feature(feat_df, col_name):\n",
    "    df_long = feat_df.stack().reset_index()\n",
    "    df_long.columns = ['Traded', 'Ticker', col_name]\n",
    "    df_long['Traded'] = df_long['Traded'].dt.normalize()\n",
    "    return df_long\n",
    "\n",
    "technical_features = [\n",
    "    (mom_5d, 'mom_5d'), \n",
    "    (mom_20d, 'mom_20d'), \n",
    "    (vol_20d, 'vol_20d'), \n",
    "    (ma_20d, 'ma_20d')\n",
    "]\n",
    "\n",
    "for feature, name in technical_features:\n",
    "    stacked_feature = stack_feature(feature, name)\n",
    "    merged = pd.merge(merged, stacked_feature, on=['Traded','Ticker'], how='left')\n",
    "\n",
    "merged = merged.dropna(subset=['mom_5d','mom_20d','vol_20d','ma_20d'])\n",
    "\n",
    "# ====================================\n",
    "# STEP 3: ENHANCE SENTIMENT FEATURES\n",
    "# ====================================\n",
    "merged = merged.sort_values(['Ticker', 'Traded'])\n",
    "\n",
    "# Add lagging (3-day shift) for net_sentiment and weighted_sentiment\n",
    "merged['net_sentiment_lag3'] = merged.groupby('Ticker')['net_sentiment'].shift(3)\n",
    "merged['weighted_sentiment_lag3'] = merged.groupby('Ticker')['weighted_sentiment'].shift(3)\n",
    "\n",
    "# 30-day rolling means for sentiment\n",
    "merged['net_sentiment_rolling30'] = merged.groupby('Ticker')['net_sentiment'].transform(lambda x: x.rolling(30).mean())\n",
    "merged['weighted_sentiment_rolling30'] = merged.groupby('Ticker')['weighted_sentiment'].transform(lambda x: x.rolling(30).mean())\n",
    "merged['net_sentiment_D_rolling30'] = merged.groupby('Ticker')['net_sentiment_D'].transform(lambda x: x.rolling(30).mean())\n",
    "merged['net_sentiment_R_rolling30'] = merged.groupby('Ticker')['net_sentiment_R'].transform(lambda x: x.rolling(30).mean())\n",
    "\n",
    "# 30-day rolling sum for total_trades\n",
    "merged['total_trades_rolling30'] = merged.groupby('Ticker')['total_trades'].transform(lambda x: x.rolling(30).sum())\n",
    "\n",
    "# Drop rows that may now have NaN due to rolling/lagging\n",
    "merged = merged.dropna(subset=[\n",
    "    'net_sentiment_lag3', 'weighted_sentiment_lag3',\n",
    "    'net_sentiment_rolling30', 'weighted_sentiment_rolling30',\n",
    "    'total_trades_rolling30', 'net_sentiment_D_rolling30', 'net_sentiment_R_rolling30'\n",
    "])\n",
    "\n",
    "# ====================================\n",
    "# STEP 4: SELECT FEATURES AND TARGET\n",
    "# ====================================\n",
    "# Focus more on sentiment features now and keep minimal technical\n",
    "feature_cols = [\n",
    "    'net_sentiment', 'weighted_sentiment', 'net_sentiment_D', 'net_sentiment_R',\n",
    "    'total_trades', 'avg_trade_size',\n",
    "    'mom_5d', 'mom_20d', 'vol_20d', 'ma_20d',\n",
    "    'net_sentiment_lag3', 'weighted_sentiment_lag3',\n",
    "    'net_sentiment_rolling30', 'weighted_sentiment_rolling30',\n",
    "    'total_trades_rolling30', 'net_sentiment_D_rolling30', 'net_sentiment_R_rolling30'\n",
    "]\n",
    "target_col = 'Future_Return'\n",
    "\n",
    "X = merged[feature_cols]\n",
    "y = merged[target_col]\n",
    "\n",
    "# ====================================\n",
    "# STEP 5: TIME SERIES SPLIT AND SCALING\n",
    "# ====================================\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# ====================================\n",
    "# STEP 6: MODEL TRAINING\n",
    "# ====================================\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [5, 10],\n",
    "    'min_samples_leaf': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'max_features': ['sqrt', None],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring='r2', cv=tscv, n_jobs=-1, verbose=1, error_score='raise')\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "print(\"Best Params:\", grid_search.best_params_)\n",
    "print(\"Best CV R^2 Score:\", grid_search.best_score_)\n",
    "\n",
    "# ====================================\n",
    "# STEP 7: FINAL MODEL EVALUATION\n",
    "# ====================================\n",
    "cutoff_date = pd.to_datetime(\"2022-12-31\")\n",
    "merged['Traded'] = merged['Traded'].dt.tz_localize(None)\n",
    "train_mask = merged['Traded'] < cutoff_date\n",
    "test_mask = merged['Traded'] >= cutoff_date\n",
    "\n",
    "X_train, y_train = X[train_mask], y[train_mask]\n",
    "X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "final_model = RandomForestRegressor(**grid_search.best_params_, random_state=42)\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "y_pred = final_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Test R^2 Score with more sentiment:\", r2_score(y_test, y_pred))\n",
    "print(\"Feature Importances:\", dict(zip(feature_cols, final_model.feature_importances_)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portfolio Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# STEP 1: GENERATE PREDICTED RETURNS\n",
    "# ====================================\n",
    "y_pred = final_model.predict(X_test_scaled)\n",
    "\n",
    "predicted_returns = pd.DataFrame({\n",
    "    'Ticker': merged[test_mask]['Ticker'].values,\n",
    "    'Traded': merged[test_mask]['Traded'].values,\n",
    "    'Predicted_Return': y_pred\n",
    "})\n",
    "\n",
    "mean_predicted_returns = predicted_returns.groupby('Ticker')['Predicted_Return'].mean()\n",
    "mean_predicted_returns = mean_predicted_returns.sort_values(ascending=False)\n",
    "\n",
    "print(\"\\nTop Predicted Returns (Mean per Stock):\")\n",
    "print(mean_predicted_returns.head(30))\n",
    "\n",
    "# ====================================\n",
    "# STEP 2: SELECT TOP-K STOCKS\n",
    "# ====================================\n",
    "k = 30  # Number of top stocks\n",
    "top_k_tickers = mean_predicted_returns.head(k).index.tolist()\n",
    "print(f\"\\nTop {k} stocks selected for the portfolio:\\n{top_k_tickers}\")\n",
    "\n",
    "# ====================================\n",
    "# STEP 3: HISTORICAL RETURNS AND COVARIANCE\n",
    "# ====================================\n",
    "historical_returns = pivoted_close[top_k_tickers].pct_change().dropna()\n",
    "cov_matrix = historical_returns.cov()\n",
    "expected_returns = mean_predicted_returns[top_k_tickers].values\n",
    "\n",
    "# ====================================\n",
    "# STEP 4: PORTFOLIO OPTIMIZATION (with min weight constraint)\n",
    "# ====================================\n",
    "def negative_sharpe_ratio(weights, expected_returns, cov_matrix, risk_free_rate=0.0):\n",
    "    portfolio_return = np.dot(weights, expected_returns)\n",
    "    portfolio_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))\n",
    "    return -(portfolio_return - risk_free_rate) / portfolio_volatility\n",
    "\n",
    "# Set minimum weight constraint\n",
    "min_w = 0.02  # each asset at least 2%\n",
    "if k * min_w > 1:\n",
    "    raise ValueError(\"Minimum weight per asset too large; not feasible.\")\n",
    "\n",
    "# Constraints:\n",
    "# 1) Weights sum to 1\n",
    "eq_constraints = {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}\n",
    "\n",
    "# 2) Each weight >= min_w\n",
    "ineq_constraints = []\n",
    "for i in range(k):\n",
    "    ineq_constraints.append({'type': 'ineq', 'fun': lambda x, i=i: x[i] - min_w})\n",
    "\n",
    "constraints = [eq_constraints] + ineq_constraints\n",
    "\n",
    "bounds = tuple((0, 1) for _ in range(k))\n",
    "\n",
    "# Initial guess: allocate min_w to each, distribute remainder equally\n",
    "remaining = 1 - k * min_w\n",
    "if remaining < 0:\n",
    "    raise ValueError(\"No feasible solution: sum of min weights exceeds 1.\")\n",
    "initial_weights = np.full(k, min_w) + (remaining / k)\n",
    "\n",
    "result = minimize(\n",
    "    negative_sharpe_ratio, \n",
    "    initial_weights, \n",
    "    args=(expected_returns, cov_matrix),\n",
    "    method='SLSQP',\n",
    "    bounds=bounds,\n",
    "    constraints=constraints\n",
    ")\n",
    "\n",
    "optimal_weights = result.x\n",
    "\n",
    "# ====================================\n",
    "# STEP 5: PORTFOLIO SUMMARY\n",
    "# ====================================\n",
    "portfolio_return = np.dot(optimal_weights, expected_returns)\n",
    "portfolio_volatility = np.sqrt(np.dot(optimal_weights.T, np.dot(cov_matrix, optimal_weights)))\n",
    "sharpe_ratio = portfolio_return / portfolio_volatility\n",
    "\n",
    "print(\"\\nOptimal Portfolio Weights:\")\n",
    "for ticker, weight in zip(top_k_tickers, optimal_weights):\n",
    "    print(f\"{ticker}: {weight:.4f}\")\n",
    "\n",
    "print(\"\\nPortfolio Performance:\")\n",
    "print(f\"Expected Portfolio Return: {portfolio_return:.4%}\")\n",
    "print(f\"Portfolio Volatility: {portfolio_volatility:.4%}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pie Chart\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.pie(optimal_weights, \n",
    "        labels=[f'{ticker}\\n{weight:.1%}' for ticker, weight in zip(top_k_tickers, optimal_weights)],\n",
    "        autopct='%1.1f%%',\n",
    "        pctdistance=0.85)\n",
    "plt.title('Portfolio Allocation (Pie Chart)')\n",
    "plt.show()\n",
    "\n",
    "# Print Summary Statistics\n",
    "print(\"\\nPortfolio Statistics:\")\n",
    "print(f\"Number of stocks: {len(top_k_tickers)}\")\n",
    "print(f\"Largest allocation: {max(optimal_weights):.1%} ({top_k_tickers[np.argmax(optimal_weights)]})\")\n",
    "print(f\"Smallest allocation: {min(optimal_weights):.1%} ({top_k_tickers[np.argmin(optimal_weights)]})\")\n",
    "print(f\"Average allocation: {np.mean(optimal_weights):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the cutoff date for analysis (end of 2024)\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "# Slice historical_returns up to end_date if needed\n",
    "historical_returns_2024 = historical_returns.loc[:end_date]\n",
    "\n",
    "# Get S&P 500 data only up to 2024\n",
    "sp500 = yf.download('^GSPC', \n",
    "                    start=historical_returns_2024.index[0], \n",
    "                    end=end_date)['Adj Close']\n",
    "sp500.index = sp500.index.tz_localize(None)  # Remove timezone info\n",
    "\n",
    "# Calculate daily portfolio returns for the truncated period\n",
    "portfolio_daily_returns = pd.Series(\n",
    "    np.dot(historical_returns_2024, optimal_weights), \n",
    "    index=historical_returns_2024.index.tz_localize(None)\n",
    ")\n",
    "\n",
    "# Calculate cumulative returns for the truncated period\n",
    "portfolio_cumulative = (1 + portfolio_daily_returns).cumprod()\n",
    "\n",
    "sp500_returns = sp500.pct_change().dropna()\n",
    "sp500_cumulative = (1 + sp500_returns).cumprod()\n",
    "\n",
    "# Performance comparison plot (up to end of 2024)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(portfolio_cumulative, label='Optimized Portfolio')\n",
    "plt.plot(sp500_cumulative, label='S&P 500')\n",
    "plt.title('Portfolio Performance vs S&P 500 (Up to 2024)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Cumulative Return')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "def calculate_metrics(returns):\n",
    "    \"\"\"Calculate performance metrics.\"\"\"\n",
    "    if isinstance(returns, np.ndarray):\n",
    "        returns = pd.Series(returns)\n",
    "        \n",
    "    annual_return = np.mean(returns) * 252\n",
    "    annual_volatility = np.std(returns) * np.sqrt(252)\n",
    "    sharpe_ratio = annual_return / annual_volatility\n",
    "    \n",
    "    cum_returns = (1 + returns).cumprod()\n",
    "    rolling_max = cum_returns.expanding().max()\n",
    "    drawdowns = cum_returns/rolling_max - 1\n",
    "    max_drawdown = drawdowns.min()\n",
    "    \n",
    "    return {\n",
    "        'Annual Return': annual_return,\n",
    "        'Max Drawdown': max_drawdown\n",
    "    }\n",
    "\n",
    "# Calculate metrics for truncated period\n",
    "portfolio_metrics = calculate_metrics(portfolio_daily_returns)\n",
    "sp500_metrics = calculate_metrics(sp500_returns)\n",
    "\n",
    "# Display metrics comparison\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Optimized Portfolio': portfolio_metrics,\n",
    "    'S&P 500': sp500_metrics\n",
    "})\n",
    "print(\"\\nPerformance Metrics (Up to 2024):\")\n",
    "print(metrics_df.round(4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "joblib.dump(final_model, 'final_model.pkl')\n",
    "print(\"Model saved successfully as 'final_model.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying different ML techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Classification Task (Up/Down)\n",
    "2. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Convert the regression target to a classification target (up/down).\n",
    "# 2) Introduce a lag in sentiment features.\n",
    "\n",
    "# Introduce a lag in sentiment features to simulate that the market reacts after some delay:\n",
    "lag_days = 3\n",
    "for col in ['net_sentiment', 'weighted_sentiment', 'net_sentiment_D', 'net_sentiment_R', 'total_trades', 'avg_trade_size']:\n",
    "    merged[col] = merged.groupby('Ticker')[col].shift(lag_days)\n",
    "\n",
    "# Drop rows with NaN after shifting\n",
    "merged = merged.dropna(subset=['Future_Return'] + ['net_sentiment','weighted_sentiment','net_sentiment_D','net_sentiment_R','total_trades','avg_trade_size','mom_5d','mom_20d','vol_20d','ma_20d'])\n",
    "\n",
    "# Convert to classification: 1 if Future_Return > 0, else 0\n",
    "merged['Future_Up'] = (merged['Future_Return'] > 0).astype(int)\n",
    "\n",
    "feature_cols = [\n",
    "    'net_sentiment',\n",
    "    'weighted_sentiment',\n",
    "    'net_sentiment_D',\n",
    "    'net_sentiment_R',\n",
    "    'total_trades',\n",
    "    'avg_trade_size',\n",
    "    'mom_5d',\n",
    "    'mom_20d',\n",
    "    'vol_20d',\n",
    "    'ma_20d'\n",
    "]\n",
    "\n",
    "X = merged[feature_cols]\n",
    "y = merged['Future_Up']\n",
    "\n",
    "# Time series split\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5, 10, None],\n",
    "    'min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "grid_search = GridSearchCV(\n",
    "    clf,\n",
    "    param_grid,\n",
    "    scoring='accuracy',  # or 'roc_auc'\n",
    "    cv=tscv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "print(\"Best Params:\", grid_search.best_params_)\n",
    "print(\"Best CV Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Final evaluation on an out-of-sample period\n",
    "cutoff_date = pd.to_datetime(\"2022-12-31\")\n",
    "merged['Traded'] = pd.to_datetime(merged['Traded']).dt.normalize()\n",
    "\n",
    "train_mask = merged['Traded'] < cutoff_date\n",
    "test_mask = merged['Traded'] >= cutoff_date\n",
    "\n",
    "X_train, y_train = X[train_mask], y[train_mask]\n",
    "X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "final_clf = RandomForestClassifier(**grid_search.best_params_, random_state=42)\n",
    "final_clf.fit(X_train, y_train)\n",
    "y_pred = final_clf.predict(X_test)\n",
    "y_prob = final_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Test AUC:\", roc_auc_score(y_test, y_prob))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# STEP 1: PREPARE DATA\n",
    "# ====================================\n",
    "\n",
    "# We'll use a shorter horizon (5 days) for future returns:\n",
    "future_horizon = 5\n",
    "future_returns = pivoted_close.shift(-future_horizon) / pivoted_close - 1.0\n",
    "\n",
    "# Melt future_returns to long format\n",
    "future_returns_long = future_returns.stack().reset_index()\n",
    "future_returns_long.columns = ['Traded', 'Ticker', 'Future_Return']\n",
    "\n",
    "# Ensure ticker and date formatting matches sentiment\n",
    "if not all(col in daily_sentiment.columns for col in ['Traded','Ticker']):\n",
    "    sentiment_df = daily_sentiment.reset_index()\n",
    "else:\n",
    "    sentiment_df = daily_sentiment.copy()\n",
    "\n",
    "sentiment_df['Ticker'] = sentiment_df['Ticker'].str.upper().str.strip()\n",
    "future_returns_long['Ticker'] = future_returns_long['Ticker'].str.upper().str.strip()\n",
    "\n",
    "# Filter sentiment to ensure overlap (adjust date as needed)\n",
    "sentiment_df = sentiment_df[sentiment_df['Traded'] >= '2016-01-01']\n",
    "\n",
    "# Normalize/truncate times\n",
    "sentiment_df['Traded'] = sentiment_df['Traded'].dt.normalize()\n",
    "future_returns_long['Traded'] = future_returns_long['Traded'].dt.normalize()\n",
    "\n",
    "# Merge\n",
    "merged = pd.merge(sentiment_df, future_returns_long, on=['Traded', 'Ticker'], how='inner').dropna(subset=['Future_Return'])\n",
    "\n",
    "if merged.empty:\n",
    "    print(\"Merged dataframe is empty. Check ticker/date overlap before proceeding.\")\n",
    "else:\n",
    "    # ====================================\n",
    "    # STEP 2: ADD ADDITIONAL PRICE FEATURES\n",
    "    # ====================================\n",
    "    price_features = pivoted_close.copy()\n",
    "\n",
    "    # Daily returns\n",
    "    daily_ret = price_features.pct_change()\n",
    "\n",
    "    # 5-day momentum\n",
    "    mom_5d = daily_ret.rolling(window=5).mean()\n",
    "\n",
    "    # 20-day momentum\n",
    "    mom_20d = daily_ret.rolling(window=20).mean()\n",
    "\n",
    "    # 20-day volatility\n",
    "    vol_20d = daily_ret.rolling(window=20).std()\n",
    "\n",
    "    # 20-day moving average price\n",
    "    ma_20d = price_features.rolling(window=20).mean()\n",
    "\n",
    "    def stack_feature(feat_df, col_name):\n",
    "        df_long = feat_df.stack().reset_index()\n",
    "        df_long.columns = ['Traded', 'Ticker', col_name]\n",
    "        df_long['Traded'] = df_long['Traded'].dt.normalize()\n",
    "        return df_long\n",
    "\n",
    "    mom_5d_long = stack_feature(mom_5d, 'mom_5d')\n",
    "    mom_20d_long = stack_feature(mom_20d, 'mom_20d')\n",
    "    vol_20d_long = stack_feature(vol_20d, 'vol_20d')\n",
    "    ma_20d_long = stack_feature(ma_20d, 'ma_20d')\n",
    "\n",
    "    # Merge all additional features into merged\n",
    "    merged = pd.merge(merged, mom_5d_long, on=['Traded','Ticker'], how='left')\n",
    "    merged = pd.merge(merged, mom_20d_long, on=['Traded','Ticker'], how='left')\n",
    "    merged = pd.merge(merged, vol_20d_long, on=['Traded','Ticker'], how='left')\n",
    "    merged = pd.merge(merged, ma_20d_long, on=['Traded','Ticker'], how='left')\n",
    "\n",
    "    # Drop rows with no price features\n",
    "    merged = merged.dropna(subset=['mom_5d','mom_20d','vol_20d','ma_20d'])\n",
    "\n",
    "    # ====================================\n",
    "    # STEP 3: SELECT FEATURES AND TARGET\n",
    "    # ====================================\n",
    "    feature_cols = [\n",
    "        'net_sentiment',\n",
    "        'weighted_sentiment',\n",
    "        'net_sentiment_D',\n",
    "        'net_sentiment_R',\n",
    "        'total_trades',\n",
    "        'avg_trade_size',\n",
    "        'mom_5d',\n",
    "        'mom_20d',\n",
    "        'vol_20d',\n",
    "        'ma_20d'\n",
    "    ]\n",
    "    target_col = 'Future_Return'\n",
    "\n",
    "    X = merged[feature_cols]\n",
    "    y = merged[target_col]\n",
    "\n",
    "    # ====================================\n",
    "    # STEP 4: TIME SERIES SPLIT\n",
    "    # ====================================\n",
    "    tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "    # ====================================\n",
    "    # STEP 5: HYPERPARAMETER TUNING WITH XGBOOST\n",
    "    # ====================================\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [3, 5, 10],\n",
    "        'learning_rate': [0.01, 0.05, 0.1]\n",
    "    }\n",
    "\n",
    "    model = XGBRegressor(random_state=42, tree_method='hist', n_jobs=-1)  # Use hist for faster training if large dataset\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        model, \n",
    "        param_grid, \n",
    "        scoring='r2', \n",
    "        cv=tscv, \n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    print(\"Best Params:\", grid_search.best_params_)\n",
    "    print(\"Best CV R^2 Score:\", grid_search.best_score_)\n",
    "\n",
    "    # Train final model on entire dataset before cutoff, then test on a hold-out\n",
    "    cutoff_date = pd.to_datetime(\"2022-12-31\")\n",
    "    # Ensure Traded is naive datetime\n",
    "    merged['Traded'] = merged['Traded'].dt.tz_localize(None)\n",
    "\n",
    "    train_mask = merged['Traded'] < cutoff_date\n",
    "    test_mask = merged['Traded'] >= cutoff_date\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "    final_model = XGBRegressor(**grid_search.best_params_, random_state=42, tree_method='hist', n_jobs=-1)\n",
    "    final_model.fit(X_train, y_train)\n",
    "    y_pred = final_model.predict(X_test)\n",
    "\n",
    "    print(\"Test R^2 Score with improved setup:\", r2_score(y_test, y_pred))\n",
    "\n",
    "    # Feature importances\n",
    "    importances = final_model.feature_importances_\n",
    "    print(\"Feature Importances:\", dict(zip(feature_cols, importances)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
